

from google.colab import drive
drive.mount('/content/drive')

import os

!cp -r "/content/drive/MyDrive/Untitled folder/train" "/content/train_data"
!cp -r "/content/drive/MyDrive/Untitled folder/Test" "/content/test_data"

print("Train folders:", os.listdir("/content/train_data"))
print("Test files:", len(os.listdir("/content/test_data")))


import numpy as np
import pandas as pd
import librosa
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split

from sklearn.preprocessing import LabelEncoder, StandardScaler



TRAIN_PATH = "/content/train_data"
TEST_PATH = "/content/test_data"
SAMPLE_SUBMISSION_PATH = "sample_submission.csv"




def extract_features(file_path, max_len=22050 * 3):
    try:
        y, sr = librosa.load(file_path, sr=22050)

        if len(y) > max_len:
            y = y[:max_len]
        else:
            y = np.pad(y, (0, max_len - len(y)))

        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
        return np.mean(mfcc.T, axis=0)

    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        return np.zeros(40)



files, labels = [], []

for label_name in os.listdir(TRAIN_PATH):
    class_dir = os.path.join(TRAIN_PATH, label_name)
    if os.path.isdir(class_dir):
        for f in os.listdir(class_dir):
            if f.endswith(".wav"):
                files.append(os.path.join(class_dir, f))
                labels.append(label_name)

print(f"Found {len(files)} training files across {len(set(labels))} classes")




X, y = [], []

for f, label in tqdm(zip(files, labels), total=len(files), desc="Extracting features"):
    X.append(extract_features(f))
    y.append(label)

X = np.array(X)
y = np.array(y)



scaler = StandardScaler()
X = scaler.fit_transform(X)

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
class_names = label_encoder.classes_

print("Classes:", class_names)



class AudioDataset(Dataset):
    def __init__(self, X, y=None):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.long) if y is not None else None

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        if self.y is not None:
            return self.X[idx], self.y[idx]
        return self.X[idx]




full_dataset = AudioDataset(X, y)
train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size

train_ds, val_ds = random_split(full_dataset, [train_size, val_size])

train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)




class AudioNet(nn.Module):
    def __init__(self, input_dim, num_classes):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.4),

            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        return self.net(x)




device = "cuda" if torch.cuda.is_available() else "cpu"
model = AudioNet(X.shape[1], len(class_names)).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)



def accuracy(loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for Xb, yb in loader:
            Xb, yb = Xb.to(device), yb.to(device)
            preds = model(Xb)
            _, predicted = torch.max(preds, 1)
            total += yb.size(0)
            correct += (predicted == yb).sum().item()
    return 100 * correct / total




best_val_acc = 0
best_model_path = "best_model.pth"

for epoch in range(50):
    model.train()
    running_loss = 0

    for Xb, yb in train_loader:
        Xb, yb = Xb.to(device), yb.to(device)

        optimizer.zero_grad()
        preds = model(Xb)
        loss = criterion(preds, yb)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    train_acc = accuracy(train_loader)
    val_acc = accuracy(val_loader)

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), best_model_path)

    print(
        f"Epoch [{epoch+1}/50] "
        f"Loss: {running_loss/len(train_loader):.4f} "
        f"Train Acc: {train_acc:.2f}% "
        f"Val Acc: {val_acc:.2f}% "
        f"Best: {best_val_acc:.2f}%"
    )



model.load_state_dict(torch.load(best_model_path))
print(f"Loaded best model with Val Acc: {best_val_acc:.2f}%")


if not os.path.exists(SAMPLE_SUBMISSION_PATH):
    test_files = sorted(os.listdir(TEST_PATH))
    pd.DataFrame({
        "ID": test_files,
        "Class": [""] * len(test_files)
    }).to_csv(SAMPLE_SUBMISSION_PATH, index=False)

sample_submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)
test_ids = sample_submission["ID"].tolist()

test_features = []
for file_id in tqdm(test_ids, desc="Extracting test features"):
    file_path = os.path.join(TEST_PATH, file_id)
    if os.path.exists(file_path):
        test_features.append(extract_features(file_path))
    else:
        test_features.append(np.zeros(X.shape[1]))

test_X = scaler.transform(np.array(test_features))

test_ds = AudioDataset(test_X)
test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)



model.eval()
predictions = []

with torch.no_grad():
    for batch_X in test_loader:
        batch_X = batch_X.to(device)
        outputs = model(batch_X)
        preds = torch.argmax(outputs, dim=1)
        predictions.extend(preds.cpu().numpy())




submission = pd.DataFrame({
    "ID": test_ids,
    "Class": label_encoder.inverse_transform(predictions)
})

submission.to_csv("submission.csv", index=False)

print("submission.csv created")
print(submission.head())
print(f"Total rows: {len(submission)}")
print(f"Best validation accuracy: {best_val_acc:.2f}%")
